import joblib
import streamlit as st
import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model

st.set_page_config(page_title="Churn App", layout="wide")

def convert_to_number_or_None(value: str):
    """
    If data has blanks, this method allows user not to enter any value
    and converts it to the correct type
    :param value:
    :return: float or None
    """
    if value == "":
        result = None
    else:
        try:
            result = float(value)
        except ValueError:
            result = None
            st.warning("–í–≤–µ–¥—ñ—Ç—å —á–∏—Å–ª–æ –∞–±–æ –∑–∞–ª–∏—à—Ç–µ –ø–æ–ª–µ –ø—É—Å—Ç–∏–º")
    return result


def classification_report_message(model: str, metrics):
    """
    This method displays main metrics of the model
    :param model:
    :param metrics:
    :return:
    """
    st.subheader(f"–ü—Ä–æ –º–æ–¥–µ–ª—å {model}:")
    st.markdown(f"üí°–¢–æ—á–Ω—ñ—Å—Ç—å: **{metrics['accuracy'] * 100:.2f}%**")
    st.write(f"–ö–æ–ª–∏ –º–æ–¥–µ–ª—å –∫–∞–∂–µ, —â–æ –∫–ª—ñ—î–Ω—Ç –ù–ï –ø—ñ–¥–µ, "
             f"—É {metrics['0']['precision'] * 100:.2f}% –≤–∏–ø–∞–¥–∫—ñ–≤ —Ü–µ –ø—Ä–∞–≤–¥–∞. "
             f"–ó —É—Å—ñ—Ö –∫–ª—ñ—î–Ω—Ç—ñ–≤, —è–∫—ñ –ù–ï –π–¥—É—Ç—å, –º–æ–¥–µ–ª—å –∑–Ω–∞—Ö–æ–¥–∏—Ç—å {metrics['0']['recall'] * 100:.2f}%.")
    st.write(f"–Ø–∫—â–æ –º–æ–¥–µ–ª—å –∫–∞–∂–µ, —â–æ –∫–ª—ñ—î–Ω—Ç –ø—ñ–¥–µ, "
             f"—Ü–µ –ø—Ä–∞–≤–¥–∞ –≤ {metrics['1']['precision'] * 100:.2f}% –≤–∏–ø–∞–¥–∫—ñ–≤. "
             f"–ó —É—Å—ñ—Ö —Ä–µ–∞–ª—å–Ω–∏—Ö –∫–ª—ñ—î–Ω—Ç—ñ–≤, —è–∫—ñ –ø—ñ–¥—É—Ç—å, –º–æ–¥–µ–ª—å –∑–Ω–∞—Ö–æ–¥–∏—Ç—å {metrics['1']['recall'] * 100:.2f}%.")


# ----------------------------
# 1. Load model, medians, metrics
# ----------------------------

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
# Random Forest model
random_forest_pipeline = joblib.load("random_forest_pipeline.joblib")

#SVM model
svm_pipeline = joblib.load("churn_svm_model.pkl")

# Neural Network model
nn_model = load_model("churn_NNmodel.keras")
nn_scaler = joblib.load("scalerNN.pkl")
with open("feature_namesNN.json", "r") as f:
    nn_feature_names = json.load(f)

#To do models

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–µ–¥—ñ–∞–Ω
# Random Forest medians
rf_median = None
with open("rf_medians.json", "r") as f:
    rf_medians = json.load(f)

# SVM medians
svm_medians = None
with open("svm_medians.json", "r") as f:
    svm_medians = json.load(f)

# To do medians

#–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫
# Random Forest metrics
rf_metrics = None
with open("rf_metrics.json", "r") as f:
    rf_metrics = json.load(f)

# SVM metrics
svm_metrics = None
with open("svm_metrics.json", "r") as f:
    svm_metrics = json.load(f)

# Neural Network metrics
with open("nn_metrics.json", "r") as f:
    nn_metrics = json.load(f)

st.title("üì° –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –í—ñ–¥—Ç–æ–∫—É –ö–ª—ñ—î–Ω—Ç—ñ–≤ –¥–ª—è –¢–µ–ª–µ–∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ–π–Ω–æ—ó –∫–æ–º–ø–∞–Ω—ñ—ó")
st.write("–í–≤–µ–¥—ñ—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∫–ª—ñ—î–Ω—Ç–∞, —â–æ–± –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –≤—ñ–¥—Ç–æ–∫—É.")


# ----------------------------------------
# 2. Input fields
# ----------------------------------------

input_mode = st.radio(
    "–û–±–µ—Ä—ñ—Ç—å —Ñ–æ—Ä–º–∞—Ç –≤–≤–æ–¥—É –¥–∞–Ω–∏—Ö:",
    ["–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–ª—ñ—î–Ω—Ç–∞ (–≤–≤—ñ–¥ –¥–∞–Ω–∏—Ö –≤—Ä—É—á–Ω—É)", "–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –¥–µ–∫—ñ–ª—å–∫–æ—Ö (–∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ CSV —Ñ–∞–π–ª)"]
)

if input_mode == "–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–ª—ñ—î–Ω—Ç–∞ (–≤–≤—ñ–¥ –¥–∞–Ω–∏—Ö –≤—Ä—É—á–Ω—É)":
    # –ü–µ—Ä—à–∏–π —Ä—è–¥
    cols1 = st.columns(5)

    with cols1[0]:
        is_tv_subscriber = st.selectbox("*–ß–∏ –ø—ñ–¥–ø–∏—Å–∞–Ω–∏–π –Ω–∞ TV?", ['—Ç–∞–∫', '–Ω—ñ'])
        is_tv_subscriber = 1 if is_tv_subscriber == "—Ç–∞–∫" else 0
    with cols1[1]:
        is_movie_package_subscriber = st.selectbox("*–ß–∏ –ø—ñ–¥–ø–∏—Å–∞–Ω–∏–π –Ω–∞ –ø–∞–∫–µ—Ç —Ñ—ñ–ª—å–º—ñ–≤?", ['—Ç–∞–∫', '–Ω—ñ'])
        is_movie_package_subscriber = 1 if is_movie_package_subscriber == "—Ç–∞–∫" else 0
    with cols1[2]:
        subscription_age = st.number_input("*–¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –ø—ñ–¥–ø–∏—Å–∫–∏ (–º—ñ—Å)", 0.0, 100.0)
    with cols1[3]:
        bill_avg = st.number_input("*–°–µ—Ä–µ–¥–Ω—ñ–π —á–µ–∫ –Ω–∞ –º—ñ—Å—è—Ü—å", 0.0, 1000.0)
    with cols1[4]:
        # remaining_contract = st.number_input("–ó–∞–ª–∏—à–æ–∫ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É (–º—ñ—Å)", 0.0, 36.0)
        raw_value_remaining_contract = st.text_input("–ó–∞–ª–∏—à–æ–∫ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É (–º—ñ—Å) (—è–∫—â–æ —î)")
        remaining_contract = convert_to_number_or_None(raw_value_remaining_contract)
        if remaining_contract is None:
            remaining_contract = rf_medians['remaining_contract_median']


    # –î—Ä—É–≥–∏–π —Ä—è–¥
    cols2 = st.columns(5)

    with cols2[0]:
        service_failure_count = st.number_input("*–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–±–æ—ó–≤", 0, 100)
    with cols2[1]:
        # download_avg = st.number_input("–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (GB)", 0.0, 10000.0)
        raw_value_download_avg = st.text_input("–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (GB) (—è–∫—â–æ —î)")
        download_avg = convert_to_number_or_None(raw_value_download_avg)
        if download_avg is None:
            download_avg = rf_medians['download_median']

    with cols2[2]:
        # upload_avg = st.number_input("–ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (GB)", 0.0, 10000.0)
        raw_value_upload_avg = st.text_input("–ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (GB) (—è–∫—â–æ —î)")
        upload_avg = convert_to_number_or_None(raw_value_upload_avg)
        if upload_avg is None:
            upload_avg = rf_medians['upload_median']

    with cols2[3]:
        download_over_limit = st.number_input("\n*–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–Ω–∞–¥ –º–µ–∂—É", 0, 100)
    with cols2[4]:
        pass


elif input_mode == "–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –¥–µ–∫—ñ–ª—å–∫–æ—Ö (–∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ CSV —Ñ–∞–π–ª)":
    uploaded_file = st.file_uploader("–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ CSV —Ñ–∞–π–ª –∑ –∫–æ–ª–æ–Ω–∫–∞–º–∏ "
                                     "is_tv_subscriber,"
                                     "is_movie_package_subscriber,"
                                     "subscription_age,"
                                     "bill_avg,"
                                     "reamining_contract,"
                                     "service_failure_count,"
                                     "download_avg,upload_avg,"
                                     "download_over_limit,"
                                     "download_avg_missing,"
                                     "upload_avg_missing", type=["csv"])

    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)

        st.write("üìÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ –¥–∞–Ω—ñ:")
        st.dataframe(df)

        # –ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ, —â–æ –≤—Å—ñ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ —î
        required_cols = [
            "is_tv_subscriber",
            "is_movie_package_subscriber",
            "subscription_age",
            "bill_avg",
            "reamining_contract",
            "service_failure_count",
            "download_avg",
            "upload_avg",
            "download_over_limit"
        ]

        missing = [c for c in required_cols if c not in df.columns]

        if missing:
            st.error(f"‚ùå –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ —É CSV: {missing}")


# ----------------------------------------
# 3. Prediction
# ----------------------------------------

# –í–∏–ø–∞–¥–∞—é—á–µ –º–µ–Ω—é –¥–ª—è –≤–∏–±–æ—Ä—É –º–æ–¥–µ–ª—ñ
model_name = st.selectbox(
    "–û–±–µ—Ä—ñ—Ç—å –±—É–¥—å-—è–∫—É –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:",
    ['Random Forest', 'SVM', '–ù–µ–π—Ä–æ–Ω–Ω–∞ –º–µ—Ä–µ–∂–∞']
)


if st.button("–ü–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ –≤—ñ–¥—Ç—ñ–∫"):
    if input_mode == '–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–ª—ñ—î–Ω—Ç–∞ (–≤–≤—ñ–¥ –¥–∞–Ω–∏—Ö –≤—Ä—É—á–Ω—É)':
        # –§–æ—Ä–º—É—î–º–æ –≤—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ
        X = np.array([[
            is_tv_subscriber,
            is_movie_package_subscriber,
            subscription_age,
            bill_avg,
            remaining_contract,
            service_failure_count,
            download_avg,
            upload_avg,
            download_over_limit
        ]], dtype=float)

        st.subheader("–í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ:")
        st.write(X)

        if model_name == 'Random Forest':
            classification_report_message(model_name, rf_metrics)
            #–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
            probability = random_forest_pipeline.predict_proba(X)[0][1] * 100

        elif model_name == 'SVM':
            classification_report_message(model_name, svm_metrics)

            probability = svm_pipeline.predict_proba(X)[0][1] * 100

        elif model_name == '–ù–µ–π—Ä–æ–Ω–Ω–∞ –º–µ—Ä–µ–∂–∞':
            if nn_metrics:
                classification_report_message(model_name, nn_metrics)

                # –§–æ—Ä–º—É—î–º–æ –≤—Ö—ñ–¥ —Å–∞–º–µ –ø—ñ–¥ NN: —Å–ª–æ–≤–Ω–∏–∫ –∑ –Ω–∞–∑–≤–∞–º–∏ –æ–∑–Ω–∞–∫
            nn_input = {
                "is_tv_subscriber": is_tv_subscriber,
                "is_movie_package_subscriber": is_movie_package_subscriber,
                "subscription_age": subscription_age,
                "bill_avg": bill_avg,
                "reamining_contract": remaining_contract,
                "service_failure_count": service_failure_count,
                "download_avg": download_avg,
                "upload_avg": upload_avg,
                "download_over_limit": download_over_limit,
            }

            # DataFrame –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É –∫–æ–ª–æ–Ω–æ–∫
            nn_df = pd.DataFrame([[nn_input[col] for col in nn_feature_names]], columns=nn_feature_names)

            # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
            nn_scaled = nn_scaler.transform(nn_df)

            # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ (–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É "1" ‚Äì –∫–ª—ñ—î–Ω—Ç –ø—ñ–¥–µ)
            nn_proba = nn_model.predict(nn_scaled)[0][0]
            probability = nn_proba * 100

        # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        cols = st.columns(2)

        with cols[0]:
            st.subheader("–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:")
            st.markdown(f"üíî **–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å, —â–æ –∫–ª—ñ—î–Ω—Ç –ø—ñ–¥–µ: {probability:.2f}%**")
            st.markdown(f"üëç **–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å, —â–æ –∫–ª—ñ—î–Ω—Ç –∑–∞–ª–∏—à–∏—Ç—å—Å—è: {100-probability:.2f}%**")

            if probability > 50:
                st.error("‚ö†Ô∏è –ö–ª—ñ—î–Ω—Ç –∑ –≤–∏—Å–æ–∫–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é –ø—ñ–¥–µ.")
            else:
                st.success("‚úÖ –ö–ª—ñ—î–Ω—Ç, —Å–∫–æ—Ä—ñ—à –∑–∞ –≤—Å–µ, –∑–∞–ª–∏—à–∏—Ç—å—Å—è.")

        # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
        with cols[1]:
            fig, ax = plt.subplots(figsize=(6, 3))
            ax.bar(["–ö–ª—ñ—î–Ω—Ç –ø—ñ–¥–µ"], [probability])
            ax.bar(["–ö–ª—ñ—î–Ω—Ç –∑–∞–ª–∏—à–∏—Ç—å—Å—è"], [100-probability])
            ax.set_ylim(0, 100)
            st.pyplot(fig)


    elif input_mode == "–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –¥–µ–∫—ñ–ª—å–∫–æ—Ö (–∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ CSV —Ñ–∞–π–ª)":
        if model_name == 'Random Forest':
            classification_report_message(model_name, rf_metrics)

            probabilities = random_forest_pipeline.predict_proba(df[required_cols])[:, 1] * 100

        elif model_name == 'SVM':
            classification_report_message(model_name, svm_metrics)

            probabilities = svm_pipeline.predict_proba(df[required_cols])[:, 1] * 100

        elif model_name == '–ù–µ–π—Ä–æ–Ω–Ω–∞ –º–µ—Ä–µ–∂–∞':
            if nn_metrics:
                classification_report_message(model_name, nn_metrics)

                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —â–æ –≤—Å—ñ —Ñ—ñ—á—ñ –¥–ª—è NN —î –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ñ
            missing_nn = [c for c in nn_feature_names if c not in df.columns]
            if missing_nn:
                st.error(f"‚ùå –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ: {missing_nn}")
                st.stop()

            nn_df = df[nn_feature_names].copy()

            # –Ø–∫—â–æ –≤ –¥–∞–Ω–∏—Ö —î –ø—Ä–æ–ø—É—Å–∫–∏ ‚Äì –º–æ–∂–Ω–∞ –ø—ñ–¥—Å—Ç–∞–≤–∏—Ç–∏ –º–µ–¥—ñ–∞–Ω–∏ –∑ RF –∞–±–æ –æ–∫—Ä–µ–º—ñ –¥–ª—è NN
            nn_df = nn_df.fillna(nn_df.median(numeric_only=True))

            nn_scaled = nn_scaler.transform(nn_df)
            nn_proba = nn_model.predict(nn_scaled).ravel()
            probabilities = nn_proba * 100


        df["churn_probability"] = probabilities
        df["churn_prediction"] = pd.cut(
            df["churn_probability"],
            bins=[0, 40, 70, 100],
            labels=[
                "–ö–ª—ñ—î–Ω—Ç –∑–∞–ª–∏—à–∏—Ç—å—Å—è",
                "–°–µ—Ä–µ–¥–Ω—ñ–π —Ä–∏–∑–∏–∫ –≤—ñ–¥—Ç–æ–∫—É",
                "–í–∏—Å–æ–∫–∏–π —Ä–∏–∑–∏–∫ –≤—ñ–¥—Ç–æ–∫—É"
            ],
            include_lowest=True
        )

        st.success("–ì–æ—Ç–æ–≤–æ!")
        st.dataframe(df)

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        csv_result = df.to_csv(index=False).encode("utf-8")
        st.download_button(
            "‚¨áÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏",
            csv_result,
            "predictions.csv",
            "text/csv"
        )
